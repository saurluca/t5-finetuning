{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c11dcf5-8a18-49c6-8047-f8a78fc70d74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Fine-Tuning T5-Small on a Single GPU\n",
    "\n",
    "Fine-tuning large language models (LLMs) almost always requires multiple GPUs to be practical (or possible at all). But if you're relatively new to deep learning, or you've only trained models on single GPUs before, making the jump to distributed training on multiple GPUs and multiple nodes can be extremely challenging and more than a little frustrating.\n",
    "\n",
    "The goal of this project is to start small and gradually add complexity. So we're not going to start with a \"large language model\" at all. We're starting with a very small model called [t5-small](https://huggingface.co/t5-small). Why start with a small model if we want to train considerably larger models?\n",
    "\n",
    "- Learning about model fine-tuning is a lot less frustrating if you start from a place of less complexity and are able to get results quickly!\n",
    "- When we get to the point of training larger models on distributed systems, we're going to spend a lot of time and energy on _how_ to distribute the model, data, etc., across that system. Starting smaller lets us spend some time at the beginning focusing on the training metrics that directly relate to model performance rather than the complexity involved with distributed training. Eventually we will need both, but there's no reason to try to digest all of it all at once!\n",
    "- Starting small and then scaling up will give us a solid intuition of how, when, and why to use the various tools and techniques for training larger models or for using more compute resources to train models faster.\n",
    "\n",
    "## Fine-Tuning t5-small\n",
    "\n",
    "Our goals in this notebook are simple. We want to fine-tune the `t5-small` model and verify that its behavior has changed as a result of our fine-tuning.\n",
    "\n",
    "The [t5 (text-to-text transfer transformer) family of models](https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html) was developed by Google Research. It was presented as an advancement over BERT-style models which could output only a class label or a span of the input. t5 allows the same model, loss, and hyperparameters to be used for _any_ nlp task. t5 differs from GPT models because it is an encoder-decoder model, while GPT models are decoder-only models.\n",
    "\n",
    "t5-small is a 60 million parameter model. This is _small_: the smallest version of GPT2 has more than twice as many parameters (124M); llama2-7b, one of the most commonly-used models at the time of writing, has more than 116 times as many parameters (7B, hence the name). What does this mean for us? Parameter count strongly impacts the amount of memory required to train a model. Eleuther's [Transformer Math blog post](https://blog.eleuther.ai/transformer-math/#training) has a great overview of the memory costs associated with training models of different sizes. We'll get into this in more detail in a later notebook.\n",
    "\n",
    "## A few things to keep in mind\n",
    "\n",
    "If you're looking for a set of absolute best practices for how to train particular models, this isn't the place to find them (though I will link them when I come across them, and will try to make improvements where I can, as long as they don't come at the cost of extra complexity!). The goal is to develop a high-level understanding and intuition on model training and fine-tuning, so you can fairly quickly get to something that _works_ and then iterate to make it work _better_.\n",
    "\n",
    "## Compute used in this example\n",
    "\n",
    "I am using a `g4dn.4xlarge` AWS ec2 instance, which has a single T4 GPU with 16GB VRAM.\n",
    "\n",
    "## 1. Get the model and generate some predictions\n",
    "\n",
    "Before training the model, it helps to have some sense of its base behavior. Let's take a look. See appendix C of the [t5 paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for examples of how to format inputs for various tasks.\n",
    "\n",
    "We will be using the [transformers](https://huggingface.co/docs/transformers/index) library to load and run the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Environment Setup\n",
    "OUTPUT_DIR = \"output\"\n",
    "LOG_DIR = \"logs\"\n",
    "CACHE_DIR = \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ae82d9-587e-4f75-9b30-41f253e0fbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT License\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Sample text\n",
    "input_text = (\n",
    "    \"question: What is the deepspeed license?  context: DeepSpeed \"\n",
    "    \"is an open source deep learning optimization library for PyTorch. \"\n",
    "    \"The library is designed to reduce computing power and memory use \"\n",
    "    \"and to train large distributed models with better parallelism on \"\n",
    "    \"existing computer hardware. DeepSpeed is optimized for low latency, \"\n",
    "    \"high throughput training. It includes the Zero Redundancy Optimizer \"\n",
    "    \"(ZeRO) for training models with 1 trillion or more parameters. \"\n",
    "    \"Features include mixed precision training, single-GPU, multi-GPU, \"\n",
    "    \"and multi-node training as well as custom model parallelism. The \"\n",
    "    \"DeepSpeed source code is licensed under MIT License and available on GitHub.\"\n",
    ")\n",
    "\n",
    "# Another task you could try:\n",
    "# input_text = \"Translate English to German: The house is wonderful.\"\n",
    "\n",
    "# Encode and generate response\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)[0]\n",
    "\n",
    "# Decode and print the output text\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you understand what the above code did, great! Feel free to move on. If not, also great! Here's some suggested reading.\n",
    "\n",
    "- Learn about loading models with the Transformers library [here](https://huggingface.co/learn/nlp-course/chapter2/3). The whole course is well worth the time and will give a lot of useful background for this fine-tuning guide.\n",
    "- Learn about tokenizers from Hugging Face [here](https://huggingface.co/docs/transformers/main/en/preprocessing).\n",
    "- We'll talk a lot more about GPUs later, but the `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` line checks whether the system has a CUDA-enabled GPU and the necessary software to use it. If yes, it will use the GPU; otherwise, it will fall back to CPU. This is a common pattern when training models with PyTorch, which is the underlying framework used by the transformers library in the above code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46abb2d-7ea5-49b8-9277-ece228ecb165",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Pick a Fine-Tuning Task\n",
    "\n",
    "We want to train the model to do something it's not already capable of. Let's see if we can get it to distinguish between a few different programming languages. Is it able to do this now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fbdce04-5fe1-4d4c-bf30-f2522a2038f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df - read.csv('data.csv'\n"
     ]
    }
   ],
   "source": [
    "input_text = \"question: what programming languauage is this?  code: `df <- read.csv('data.csv'); summary(df)`\"\n",
    "\n",
    "# Encode and generate response\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)[0]\n",
    "\n",
    "# Decode and print the output text\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387cd971-5d73-4a12-a9cf-ab129f212a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Doesn't look like it! Based on the T5 paper, I couldn't identify a prompt that would elicit this behavior. Furthermore, When T5 was trained, code examples were apparently explicitly excluded. So let's see if we can add this capability.\n",
    "\n",
    "> Some pages inadvertently contained code. Since the curly bracket “{” appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket.\n",
    "\n",
    "We will use the [tiny-codes](https://huggingface.co/datasets/nampdn-ai/tiny-codes) dataset, which includes code snippets along with their corresponding language, as training data. Let's pull down the dataset and do a quick exploratory analysis. Then we will have to do some cleaning and prep to make the code usable for our purposes.\n",
    "\n",
    "## 3. Get and Explore the Data\n",
    "\n",
    "You will need to create a hugging face account and log in using the `notebook_login()` interface below in order to download the [tiny-codes dataset](https://huggingface.co/datasets/nampdn-ai/tiny-codes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7efdd00-6e31-4f70-a818-bb3250ac37a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241a531d39584457a04f6c75569d198e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d869ea4-90b4-4962-b780-520ba16fb45a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "tinycodes = load_dataset(\"nampdn-ai/tiny-codes\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what languages are included and how many examples of each are in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Get the counts of the programming_language column\n",
    "# language_counts = tinycodes[\"train\"].to_pandas()[\"programming_language\"].value_counts()\n",
    "\n",
    "# # Print the counts\n",
    "# print(language_counts)\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.bar(language_counts.index, language_counts.values, alpha=0.8)\n",
    "# plt.title(\"Programming Language Distribution\")\n",
    "# plt.ylabel(\"Number of Occurrences\", fontsize=12)\n",
    "# plt.xlabel(\"Programming Language\", fontsize=12)\n",
    "# plt.xticks(\n",
    "#     rotation=45, ha=\"right\"\n",
    "# )  # Angle the x-axis labels to 45 degrees and align them to the right\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df31d5ce-f264-4757-995b-1239a8f04bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are no major imbalances and each of these languages has plenty of examples, so we're not going to do any rebalancing. Now let's take a look at what a given example looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21523987-f15d-48a6-996d-b8aca82317a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Develop a C# program snippet to Update Low Online Shopping: Product Availability for Analysis for Experts. Incorporate if/else or switch/case statements to handle various cases related to the Privacy. Dry-run, ensure your control flow logic is clear and well-commented.',\n",
       " 'main_topic': 'Online Shopping',\n",
       " 'subtopic': 'Product Availability',\n",
       " 'adjective': 'Low',\n",
       " 'action_verb': 'Update',\n",
       " 'scenario': 'for Analysis',\n",
       " 'target_audience': 'Experts',\n",
       " 'programming_language': 'C#',\n",
       " 'common_sense_topic': 'Privacy',\n",
       " 'idx': 780696,\n",
       " 'response': 'Here\\'s some sample code that demonstrates how to update low online shopping product availability for analysis by experts while incorporating `if`/`else` or `switch`/`case` statements to handle different privacy scenarios:\\n```c#\\n// Define variables\\nstring customerName = \"John Doe\";\\nbool isMinor = false;\\nDateTime birthDate = new DateTime(1980, 2, 5);\\n\\n// Check whether customer is underage based on their date of birth\\nif (birthDate > DateTime.Today.AddYears(-18))\\n{\\n    Console.WriteLine(\"Customer is not considered a minor\");\\n}\\nelse\\n{\\n    Console.WriteLine(\"Customer is considered a minor\");\\n    isMinor = true;\\n}\\n\\n// Prompt user to enter their ZIP code\\nConsole.Write(\"Enter ZIP code: \");\\nstring zipCode = Console.ReadLine();\\n\\n// Determine which privacy scenario applies based on ZIP code\\nif (zipCode == \"XXXXX\") // Scenario 1: Customer lives within the same state as the retailer\\n{\\n    Console.WriteLine($\"Updating {customerName}\\'s information with latest purchase history...\");\\n}\\nelse if (isMinor && zipCode != \"YYYYY\") //Scenario 2: Minor customer resides outside of the state where they made purchases\\n{\\n    Console.WriteLine($\"Obtaining parental consent from {customerName}\\'s legal guardian before updating their information.\");\\n}\\nelse // All other scenarios\\n{\\n    Console.WriteLine($\"{customerName}\\'s information will remain private.\");\\n}\\n```'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinycodes[\"train\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9164f228-6bf5-47eb-983c-f40e714df4c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Given our goal of creating a language classifier, we only need the `programming_language` field and the `response`. We will also need to transform the response, extracting only the code between the backticks (omitting the language name).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e826ae7-9317-4a48-b6dc-9b2d541c5fbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def preprocess_tinycodes(\n",
    "    tinycodes: datasets.DatasetDict, max_rows: Optional[int] = 20000\n",
    "):\n",
    "    # Limit to a small subset before converting to pandas to avoid loading the entire dataset\n",
    "    dtrain = tinycodes[\"train\"]\n",
    "    if isinstance(max_rows, int) and max_rows > 0:\n",
    "        subset_size = min(max_rows, len(dtrain))\n",
    "        dtrain = dtrain.select(range(subset_size))\n",
    "\n",
    "    tinycodes_subset = dtrain.to_pandas()[[\"idx\", \"programming_language\", \"response\"]]\n",
    "\n",
    "    # Extract code from response\n",
    "    tinycodes_subset[\"code\"] = tinycodes_subset[\"response\"].str.extract(\n",
    "        r\"```[a-zA-Z+#]*\\s+([\\s\\S]*?)```\", expand=False\n",
    "    )\n",
    "    return tinycodes_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e12ba3e-7f8c-4779-9afe-628401b759e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>programming_language</th>\n",
       "      <th>response</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1230929</td>\n",
       "      <td>Python</td>\n",
       "      <td>Here is python code which determines high snee...</td>\n",
       "      <td>def determine_high_sneeze_cough_etiquette(isol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1109988</td>\n",
       "      <td>Rust</td>\n",
       "      <td>Here is a possible implementation of such a fu...</td>\n",
       "      <td>// Define a function called display_extreme_se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881323</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>Here’s some sample ruby code which implements ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>780696</td>\n",
       "      <td>C#</td>\n",
       "      <td>Here's some sample code that demonstrates how ...</td>\n",
       "      <td>// Define variables\\nstring customerName = \"Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131131</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Here is a possible implementation of this func...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1405526</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Here is one possible implementation of this al...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1399969</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>Here’s a simple ruby script which uses weather...</td>\n",
       "      <td>require 'net/http'\\nrequire 'json'\\n\\n\\ndef ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>625441</td>\n",
       "      <td>relation database and SQL</td>\n",
       "      <td>Here is a possible design of a relational data...</td>\n",
       "      <td>CREATE OR REPLACE FUNCTION get_restaurant_avg_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>1041211</td>\n",
       "      <td>Python</td>\n",
       "      <td>Here's a possible implementation of this requi...</td>\n",
       "      <td>def update_handwashing(reliability):\\n    \"\"\"U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>549325</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Here's some sample julia code which implements...</td>\n",
       "      <td># Define function to check low security consen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           idx       programming_language  \\\n",
       "0      1230929                     Python   \n",
       "1      1109988                       Rust   \n",
       "2      1881323                       Ruby   \n",
       "3       780696                         C#   \n",
       "4       131131                      Julia   \n",
       "...        ...                        ...   \n",
       "19995  1405526                      Julia   \n",
       "19996  1399969                       Ruby   \n",
       "19997   625441  relation database and SQL   \n",
       "19998  1041211                     Python   \n",
       "19999   549325                      Julia   \n",
       "\n",
       "                                                response  \\\n",
       "0      Here is python code which determines high snee...   \n",
       "1      Here is a possible implementation of such a fu...   \n",
       "2      Here’s some sample ruby code which implements ...   \n",
       "3      Here's some sample code that demonstrates how ...   \n",
       "4      Here is a possible implementation of this func...   \n",
       "...                                                  ...   \n",
       "19995  Here is one possible implementation of this al...   \n",
       "19996  Here’s a simple ruby script which uses weather...   \n",
       "19997  Here is a possible design of a relational data...   \n",
       "19998  Here's a possible implementation of this requi...   \n",
       "19999  Here's some sample julia code which implements...   \n",
       "\n",
       "                                                    code  \n",
       "0      def determine_high_sneeze_cough_etiquette(isol...  \n",
       "1      // Define a function called display_extreme_se...  \n",
       "2                                                    NaN  \n",
       "3      // Define variables\\nstring customerName = \"Jo...  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "19995                                                NaN  \n",
       "19996  require 'net/http'\\nrequire 'json'\\n\\n\\ndef ge...  \n",
       "19997  CREATE OR REPLACE FUNCTION get_restaurant_avg_...  \n",
       "19998  def update_handwashing(reliability):\\n    \"\"\"U...  \n",
       "19999  # Define function to check low security consen...  \n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = preprocess_tinycodes(tinycodes)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed30a3d4-7baf-4fda-b5fe-12ad73a4fc71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You'll notice that this is far from perfect: many examples were not captured by this regex match. Let's investigate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f95f94-5032-4e68-904c-289ec8cee25a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programming_language</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bash</th>\n",
       "      <td>18.398412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C#</th>\n",
       "      <td>21.619856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C++</th>\n",
       "      <td>42.752171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Go</th>\n",
       "      <td>40.653009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Java</th>\n",
       "      <td>38.057219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JavaScript</th>\n",
       "      <td>17.210493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Julia</th>\n",
       "      <td>21.650124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neo4j database and Cypher</th>\n",
       "      <td>53.039832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>12.251244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ruby</th>\n",
       "      <td>33.171619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rust</th>\n",
       "      <td>4.510386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TypeScript</th>\n",
       "      <td>3.258145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relation database and SQL</th>\n",
       "      <td>40.335463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                code\n",
       "programming_language                \n",
       "Bash                       18.398412\n",
       "C#                         21.619856\n",
       "C++                        42.752171\n",
       "Go                         40.653009\n",
       "Java                       38.057219\n",
       "JavaScript                 17.210493\n",
       "Julia                      21.650124\n",
       "Neo4j database and Cypher  53.039832\n",
       "Python                     12.251244\n",
       "Ruby                       33.171619\n",
       "Rust                        4.510386\n",
       "TypeScript                  3.258145\n",
       "relation database and SQL  40.335463"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the percentage of NA for each programming language in data_test\n",
    "na_percent = data_test.groupby(\"programming_language\")[\"code\"].apply(\n",
    "    lambda x: x.isna().mean() * 100\n",
    ")\n",
    "\n",
    "# Display the percentage in a dataframe\n",
    "na_percent_df = pd.DataFrame(na_percent)\n",
    "na_percent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30656538-d092-47a8-bd73-5b93664fdf99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We would generally spend a lot more time iterating on the data and trying to capture code from a much greater proportion of examples. However, in this case, we're just going to subset to those languages where less than 25% of examples are NA and proceed with the fine tuning. Let's update accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75de9586-3034-4d60-9b21-f934659cc9e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_code_from_tinycodes(\n",
    "    tinycodes: datasets.DatasetDict,\n",
    "    train_frac=0.7,\n",
    "    valid_frac=0.2,\n",
    "    max_rows: Optional[int] = 30000,\n",
    "):\n",
    "    # Subset languages to Bash, C#, JavaScript, Julia, Python, Rust, TypeScript\n",
    "    languages = [\"Bash\", \"C#\", \"JavaScript\", \"Julia\", \"Python\", \"Rust\", \"TypeScript\"]\n",
    "    tinycodes_subset = (\n",
    "        tinycodes[\"train\"]\n",
    "        .to_pandas()\n",
    "        .loc[: max_rows - 1, [\"idx\", \"programming_language\", \"response\"]]\n",
    "    )\n",
    "    tinycodes_subset = tinycodes_subset[\n",
    "        tinycodes_subset[\"programming_language\"].isin(languages)\n",
    "    ]\n",
    "    # Extract code from response\n",
    "    tinycodes_subset[\"code\"] = tinycodes_subset[\"response\"].str.extract(\n",
    "        r\"```[a-zA-Z+#]*\\s+([\\s\\S]*?)```\", expand=False\n",
    "    )\n",
    "    # Drop rows with missing code\n",
    "    tinycodes_subset = tinycodes_subset.dropna(subset=[\"code\"])\n",
    "\n",
    "    # Shuffle the data\n",
    "    tinycodes_subset = tinycodes_subset.sample(frac=1, random_state=42)\n",
    "    # Split the data into train, valid, and test sets\n",
    "    train, valid, test = np.split(\n",
    "        tinycodes_subset,\n",
    "        [\n",
    "            int(train_frac * len(tinycodes_subset)),\n",
    "            int((train_frac + valid_frac) * len(tinycodes_subset)),\n",
    "        ],\n",
    "    )\n",
    "    # Convert the pandas dataframes back to DatasetDict\n",
    "    tinycodes_subset = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_pandas(train),\n",
    "            \"valid\": datasets.Dataset.from_pandas(valid),\n",
    "            \"test\": datasets.Dataset.from_pandas(test),\n",
    "        }\n",
    "    )\n",
    "    return tinycodes_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02b4005-8612-4345-bc60-dea4329a2615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Coding/t5-finetuning/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'programming_language', 'response', 'code', '__index_level_0__'],\n",
       "        num_rows: 10018\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['idx', 'programming_language', 'response', 'code', '__index_level_0__'],\n",
       "        num_rows: 2862\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'programming_language', 'response', 'code', '__index_level_0__'],\n",
       "        num_rows: 1432\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinycodes_subset = extract_code_from_tinycodes(tinycodes)\n",
    "tinycodes_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d2f867-f421-4ccf-8b37-b88176a6bd93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "programming_language\n",
       "Rust          1720\n",
       "TypeScript    1636\n",
       "Python        1483\n",
       "JavaScript    1384\n",
       "Julia         1301\n",
       "Bash          1295\n",
       "C#            1199\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of each programming language\n",
    "tinycodes_subset[\"valid\"].to_pandas()[\"programming_language\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a769f2d0-b126-442a-a1d4-7bd7944067a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We still have a reasonably balanced training set, with between 69,000 and 92,000 examples of each language. This is likely considerably more than we need. Let's start with 10,000 examples of each language and see how far that gets us. Note that this is a fairly arbitrary number of examples, and we would spend some time calibrating the size of the training set in a real-world setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45654/2137878764.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=1000, random_state=42))\n",
      "/tmp/ipykernel_45654/2137878764.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=100, random_state=42))\n",
      "/tmp/ipykernel_45654/2137878764.py:23: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=100, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "train_sub = (\n",
    "    tinycodes_subset[\"train\"]\n",
    "    .to_pandas()\n",
    "    .groupby(\"programming_language\")\n",
    "    .apply(lambda x: x.sample(n=1000, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "tinycodes_subset[\"train\"] = datasets.Dataset.from_pandas(train_sub)\n",
    "\n",
    "valid_sub = (\n",
    "    tinycodes_subset[\"valid\"]\n",
    "    .to_pandas()\n",
    "    .groupby(\"programming_language\")\n",
    "    .apply(lambda x: x.sample(n=100, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "tinycodes_subset[\"valid\"] = datasets.Dataset.from_pandas(valid_sub)\n",
    "\n",
    "test_sub = (\n",
    "    tinycodes_subset[\"test\"]\n",
    "    .to_pandas()\n",
    "    .groupby(\"programming_language\")\n",
    "    .apply(lambda x: x.sample(n=100, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "tinycodes_subset[\"test\"] = datasets.Dataset.from_pandas(test_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0bacdc-390b-445d-8790-07e452519e57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad22f9424d76437e977433fc1b36938b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc66251d7e2442c4bd8f5bc77831b39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e8f2751a9343b889b98718c0f18708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    prefix = \"question: what programming language is this?  code:\\n\\n\"\n",
    "    inputs = [prefix + ex for ex in examples[\"code\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"programming_language\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = tinycodes_subset.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbff09c-046b-4061-8608-2d6584c33259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacd7588-d5e0-45b4-9d7e-57f89005a1c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1314/1314 08:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"t5-small\"  # Or another T5 variant\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=1e4,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=LOG_DIR,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "with mlflow.start_run():\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30d855d-387a-4814-a990-ac2753dfd663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a012e06-5012-4b6a-bb54-3f719c85d8dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inference with our fine-tuned model\n",
    "\n",
    "Before we get too far, there are a few important things to point out here:\n",
    "\n",
    "- The choice of training hyperparameters was more-or-less arbitrary. In a real-world scenario, we would do a lot more experimentation with e.g. batch size, regularization, etc.\n",
    "- Similarily, the data mix was chosen for expediency, not for suitability for the current task. If I had to venture a guess before seeing any results, I would guess that the model learned some language-identification shortcuts such as associating `def` with python or `<-` with R.\n",
    "\n",
    "The point here was to show that we can, in fact, fine-tune the model in a way that meaningfully changes its behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3e013f-b578-4bc4-911c-71c6b8db3e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: question: what programming language is this?  code: def add_a_b(a, b): return a + b\n",
      "Output: Rust\n",
      "\n",
      "Input: question: what programming language is this?  code: public class HelloWorld { public static void Main() { Console.WriteLine(\"Hello, World!\"); } }\n",
      "Output: Rust\n",
      "\n",
      "Input: question: what programming language is this?  code: #include <iostream> int main() { std::cout << \"Hello, World!\" << std::endl; return 0; }\n",
      "Output: Rust\n",
      "\n",
      "Input: question: what programming language is this?  code: println(\"Hello, World!\")\n",
      "Output: Rust\n",
      "\n",
      "Input: question: what programming language is this?  code: echo \"Hello, World!\"\n",
      "Output: Rust\n",
      "\n",
      "Input: question: what programming language is this?  code: fn main() { println!(\"Hello, World!\"); }\n",
      "Output: Rust\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load latest model checkpoint\n",
    "# List all subdirectories in OUTPUT_DIR\n",
    "subdirs = next(os.walk(OUTPUT_DIR))[1]\n",
    "\n",
    "# Filter out directories that match the checkpoint pattern\n",
    "checkpoint_dirs = [d for d in subdirs if re.match(r\"checkpoint-\\d+\", d)]\n",
    "\n",
    "# Find the latest checkpoint (highest number)\n",
    "latest_checkpoint = max(checkpoint_dirs, key=lambda d: int(d.split(\"-\")[-1]))\n",
    "\n",
    "# Complete path to the latest checkpoint\n",
    "latest_checkpoint_path = os.path.join(OUTPUT_DIR, latest_checkpoint)\n",
    "\n",
    "# Load model from the latest checkpoint\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(latest_checkpoint_path)\n",
    "\n",
    "# Load tokenizer and model from the latest checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "input_texts = [\n",
    "    \"question: what programming language is this?  code: def add_a_b(a, b): return a + b\",  # Python\n",
    "    'question: what programming language is this?  code: public class HelloWorld { public static void Main() { Console.WriteLine(\"Hello, World!\"); } }',  # C#\n",
    "    'question: what programming language is this?  code: #include <iostream> int main() { std::cout << \"Hello, World!\" << std::endl; return 0; }',  # C++\n",
    "    'question: what programming language is this?  code: println(\"Hello, World!\")',  # Julia\n",
    "    'question: what programming language is this?  code: echo \"Hello, World!\"',  # Bash\n",
    "    'question: what programming language is this?  code: fn main() { println!(\"Hello, World!\"); }',  # Rust\n",
    "]\n",
    "\n",
    "# Tokenize and encode the batch of input texts\n",
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    input_texts, padding=True, return_tensors=\"pt\", truncation=True\n",
    ").to(device)\n",
    "\n",
    "# Generate responses for the batch\n",
    "output_ids = model.generate(input_ids[\"input_ids\"], max_new_tokens=20)\n",
    "\n",
    "# Decode and print the output texts\n",
    "for i, output_id in enumerate(output_ids):\n",
    "    output_text = tokenizer.decode(output_id, skip_special_tokens=True)\n",
    "    print(f\"Input: {input_texts[i]}\")\n",
    "    print(f\"Output: {output_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98cf455-7f22-4686-95c0-ad0d7af451f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's not perfect, but it got a few of them correct and it is able to respond with valid answers in the expected format. It could not do this previously. Improving these results would likely require us to obtain more varied data. I suspect very short examples like this, for example, were not actually included in the training data and might present a bit of a challenge given that there are fewer syntactic patterns that could be used to distinguish the languages in these simple \"hello, world\" scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564f395c-8de8-4549-adc6-d17b61aad038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT\n"
     ]
    }
   ],
   "source": [
    "input_text = (\n",
    "    \"question: What is the deepspeed license?  context: DeepSpeed \"\n",
    "    \"is an open source deep learning optimization library for PyTorch. \"\n",
    "    \"The library is designed to reduce computing power and memory use \"\n",
    "    \"and to train large distributed models with better parallelism on \"\n",
    "    \"existing computer hardware. DeepSpeed is optimized for low latency, \"\n",
    "    \"high throughput training. It includes the Zero Redundancy Optimizer \"\n",
    "    \"(ZeRO) for training models with 1 trillion or more parameters. \"\n",
    "    \"Features include mixed precision training, single-GPU, multi-GPU, \"\n",
    "    \"and multi-node training as well as custom model parallelism. The \"\n",
    "    \"DeepSpeed source code is licensed under MIT License and available on GitHub.\"\n",
    ")\n",
    "\n",
    "# Encode and generate response\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)[0]\n",
    "\n",
    "# Decode and print the output text\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7c72e96-98f1-4421-950e-ba300003b5d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that whether this worked depended on the training data used. When I trained for three epochs on all of the trainin examples in the chosen subset of languages, it was no longer able to get the correct answer. However, it was able to answer correctly when I trained on only 10,000 examples for three epochs.\n",
    "\n",
    "This highlights something really important: fine-tuning on new knowledge or new tasks can make the model \"forget\" its prior training. To get around this, we need to be thoughtful about how much new data we use, how long we train for, how we evaluate the fine-tuned model, and what mixture of data we use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some Experiments\n",
    "\n",
    "Model training remains an empirical practice: there's no single set of best practices that will always yield the best results. Constant experimentation and measurement are key. Get in the habit of trying different things and seeing what works. Before moving on to the next model, I suggest trying out at least three different variations of the above training run. I focused on the data in my experimentation, but you can try anything you'd like.\n",
    "\n",
    "My experiments:\n",
    "\n",
    "- train on all available training data for the chosen subset of languages for three epochs\n",
    "- train on 10k examples of each language for three epochs\n",
    "- train on 30k examples of each language for one epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1180b26-2d86-4f25-9ee3-b4efc3200eba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we fine-tuned `t5-small` on a new task. The basic process was:\n",
    "\n",
    "- Pick a model and task\n",
    "- Pick a dataset\n",
    "- Process the dataset\n",
    "- Train the model on the processed dataset\n",
    "\n",
    "This omits plenty of important steps we'll get further into in future notebooks. For example:\n",
    "\n",
    "- tuning hyperparameters\n",
    "- thoroughly evaluating the results\n",
    "- tuning training speed/efficiency\n",
    "- monitoring the training loop (we did track the experiment with MLflow but did not configure it in any way for our specific usage)\n",
    "\n",
    "We will get into each of these in future examples.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. T5-Small on Single GPU",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
